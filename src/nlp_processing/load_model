from transformers import GPTNeoForCausalLM, AutoTokenizer

# Load the GPT-Neo model and tokenizer from the local base_model directory
model = GPTNeoForCausalLM.from_pretrained('../../models/base_model')
tokenizer = AutoTokenizer.from_pretrained('../../models/base_model')

print("Loaded GPT-Neo model from models/base_model")

# Ready for fine-tuning or inference
# Example: text generation (modify as needed)
input_text = "Your input text here"
input_ids = tokenizer.encode(input_text, return_tensors='pt')
outputs = model.generate(input_ids, max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
